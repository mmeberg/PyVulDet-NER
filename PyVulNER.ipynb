{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1486084c-d93c-45f0-8c54-d6e99efcbf38",
      "metadata": {
        "id": "1486084c-d93c-45f0-8c54-d6e99efcbf38"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "from requests_oauthlib import OAuth1Session\n",
        "from requests_oauthlib import OAuth1\n",
        "import base64\n",
        "from collections import Counter, defaultdict\n",
        "import transformers\n",
        "import random\n",
        "import datasets\n",
        "import tokenize\n",
        "import io\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import datetime"
      ],
      "metadata": {
        "id": "3mPEiWdJK0UJ"
      },
      "id": "3mPEiWdJK0UJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "59160c22-99cb-4800-b320-39421a5ed7d4",
      "metadata": {
        "tags": [],
        "id": "59160c22-99cb-4800-b320-39421a5ed7d4"
      },
      "source": [
        "# Data Collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6345c9-4a02-4e8a-ac22-b17767d1938f",
      "metadata": {
        "id": "8d6345c9-4a02-4e8a-ac22-b17767d1938f"
      },
      "outputs": [],
      "source": [
        "# GitHub Access Token\n",
        "with open('github_token', 'r') as accestoken:\n",
        "    access = accestoken.readline().replace(\"\\n\",\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1573991-8a7b-4470-978e-fb8390c22a66",
      "metadata": {
        "id": "f1573991-8a7b-4470-978e-fb8390c22a66"
      },
      "outputs": [],
      "source": [
        "%run -i commits_from_GitHub.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74408f4b-311e-4b5a-8024-29043b109ce8",
      "metadata": {
        "id": "74408f4b-311e-4b5a-8024-29043b109ce8"
      },
      "outputs": [],
      "source": [
        "%run -i commit_diffs.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db6aab37-7682-462e-bc5d-4afbad18b578",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "db6aab37-7682-462e-bc5d-4afbad18b578"
      },
      "outputs": [],
      "source": [
        "%run -i diff_commit_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dad427f-b2a1-4215-bb6c-2337b4ae7b52",
      "metadata": {
        "tags": [],
        "id": "6dad427f-b2a1-4215-bb6c-2337b4ae7b52"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823eb40e-818f-4bf5-b1c1-85423707b242",
      "metadata": {
        "tags": [],
        "id": "823eb40e-818f-4bf5-b1c1-85423707b242"
      },
      "source": [
        "## Imports and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c0e06b-f5a9-49da-b1c2-60315d42e9a8",
      "metadata": {
        "id": "13c0e06b-f5a9-49da-b1c2-60315d42e9a8"
      },
      "outputs": [],
      "source": [
        "def tag_vulparts(token_list, mode, tester):\n",
        "    ner_tag_dict_nums = {'rce':[1, 2],\n",
        "                         'oob':[3, 4],\n",
        "                         'xss':[5, 6],\n",
        "                         'sql':[7, 8],\n",
        "                         'iiv':[9,10],\n",
        "                         'pat':[11,12],\n",
        "                        }\n",
        "\n",
        "    num1 = ner_tag_dict_nums[mode][0]\n",
        "    num2 = ner_tag_dict_nums[mode][1]\n",
        "\n",
        "    ner_tags = []\n",
        "    for idx, tok in enumerate(token_list):\n",
        "        if idx == tester[0]:\n",
        "            ner_tags.append(num1)\n",
        "        elif idx in range(tester[0], tester[1]+1):\n",
        "            ner_tags.append(num2)\n",
        "        else:\n",
        "            ner_tags.append(0)\n",
        "\n",
        "    return ner_tags"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getneutralText(text):\n",
        "    newtext = ''\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) > 0:\n",
        "            if line[0] == \"-\":\n",
        "                continue\n",
        "            elif line[0] == \"+\":\n",
        "                newtext = newtext + '\\n' + line[1:]\n",
        "            else:\n",
        "                newtext = newtext + '\\n' + line\n",
        "    return newtext"
      ],
      "metadata": {
        "id": "NQvlvgdtdoVk"
      },
      "id": "NQvlvgdtdoVk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_vulparts_v2(token_list, mode, tester, token_dict):\n",
        "    ner_tag_dict_nums = {'rce':[1, 2],\n",
        "                         'oob':[3, 4],\n",
        "                         'xss':[5, 6],\n",
        "                         'sql':[7, 8],\n",
        "                         'iiv':[9,10],\n",
        "                         'pat':[11,12],\n",
        "                        }\n",
        "\n",
        "    num1 = ner_tag_dict_nums[mode][0]\n",
        "    num2 = ner_tag_dict_nums[mode][1]\n",
        "    word_ids = token_dict.word_ids()\n",
        "    ner_tags = []\n",
        "    i1s = []\n",
        "    i1s_idx = []\n",
        "    for idx, tok in enumerate(token_list):\n",
        "        if idx == tester[0]:\n",
        "            ner_tags.append(num1)\n",
        "            b = word_ids[idx]\n",
        "            b_idx = idx\n",
        "        elif idx in range(tester[0]+1, tester[1]+1):\n",
        "            ner_tags.append(num2)\n",
        "            i1s.append(word_ids[idx])\n",
        "            i1s_idx.append(idx)\n",
        "        else:\n",
        "            ner_tags.append(0)\n",
        "    if b in i1s:\n",
        "        for i, v in enumerate(i1s):\n",
        "            if v == b:\n",
        "                ner_tags[i1s_idx[i]] = num1\n",
        "    return ner_tags"
      ],
      "metadata": {
        "id": "uzNSDMVVdWAS"
      },
      "id": "uzNSDMVVdWAS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shorten_data_with_windows_v2(infodict):\n",
        "    '''\n",
        "    taking the really large samples and making them smaller\n",
        "    '''\n",
        "    new_info = []\n",
        "    text = infodict['text']\n",
        "    new_size = 3000\n",
        "\n",
        "    if 'cdef ' in text[:50]:\n",
        "      return []\n",
        "    if len(text) > 500000:\n",
        "      return []\n",
        "    # neutral parts\n",
        "    if infodict.type == 'gp':\n",
        "      if len(text) >= new_size:\n",
        "          chunk_start = 0\n",
        "          chunk_size = new_size\n",
        "          breaks = re.finditer(r\"[\\n]|$|[)]|[}]|[]]\",text)\n",
        "          break_ids = [m.end(0) for m in breaks]\n",
        "          break_ids_to_use = []\n",
        "          for i in range(math.ceil(len(text)/new_size)):\n",
        "            if i == 0:\n",
        "              break_id = [m for m in break_ids if m <=new_size][-1]\n",
        "              chunk_start = 0\n",
        "            else:\n",
        "              break_id = [m for m in break_ids if m >= break_ids_to_use[i-1] and m <= break_ids_to_use[i-1]+new_size][-1]\n",
        "              chunk_start = break_ids_to_use[i-1]\n",
        "            break_ids_to_use.append(break_id)\n",
        "            chunk = [chunk_start, break_id]\n",
        "            new_tokens = text[chunk[0]:chunk[1]]\n",
        "            new_info.append(new_tokens)\n",
        "\n",
        "      else:\n",
        "        ## If there are not more than 512 tokens\n",
        "        new_tokens = text\n",
        "        new_info.append(new_tokens)\n",
        "\n",
        "    else: #vulparts\n",
        "        bp_len = infodict['bp_len']\n",
        "        bp_index = infodict['bp_index']+1\n",
        "        bp_end = bp_index+bp_len-1\n",
        "        tag_indeces = [bp_index, bp_end+1]\n",
        "\n",
        "        # If there are more than 512 tokens\n",
        "        if len(text) >= new_size:\n",
        "          # If the indeces start and stop before 512, cut the list to 512\n",
        "          if tag_indeces[0] < new_size and tag_indeces[1] <= new_size:\n",
        "            breaks = re.finditer(r\"[\\n]|$|[)]|[}]|[]]\",text)\n",
        "            break_ids = [m.end(0) for m in breaks]\n",
        "            break_id_to_use = [m for m in break_ids if m <= new_size][-1]\n",
        "            new_tokens = text[:break_id_to_use]\n",
        "            new_info.append(new_tokens)\n",
        "          elif bp_len <= new_size:\n",
        "            breaks = re.finditer(r\"[\\n]|$|[)]|[}]|[]]\",text)\n",
        "            break_ids = [m.end(0) for m in breaks]\n",
        "            break_id_to_use1 = [m for m in break_ids if m <= bp_index][-1]\n",
        "            break_id_to_use2 = [m for m in break_ids if m >= break_id_to_use1 and m <= break_id_to_use1+new_size][-1]\n",
        "            new_tokens = text[break_id_to_use1:break_id_to_use2]\n",
        "            new_info.append(new_tokens)\n",
        "          else:\n",
        "            breaks = re.finditer(r\"[\\n]|$|[)]|[}]|[]]\",text)\n",
        "            break_ids = [m.end(0) for m in breaks]\n",
        "            break_ids_to_use = []\n",
        "            for i in range(math.ceil(len(text)/new_size)):\n",
        "              if i == 0:\n",
        "                break_id = [m for m in break_ids if m <=new_size][-1]\n",
        "                chunk_start = 0\n",
        "              else:\n",
        "                break_id = [m for m in break_ids if m >= break_ids_to_use[i-1] and m <= break_ids_to_use[i-1]+new_size][-1]\n",
        "                chunk_start = break_ids_to_use[i-1]\n",
        "              break_ids_to_use.append(break_id)\n",
        "              chunk = [chunk_start, break_id]\n",
        "              new_tokens = text[chunk[0]:chunk[1]]\n",
        "              new_info.append(new_tokens)\n",
        "\n",
        "        else:\n",
        "            ## If there are not more than 512 tokens\n",
        "            new_tokens = text\n",
        "            new_info.append(new_tokens)\n",
        "\n",
        "    return new_info"
      ],
      "metadata": {
        "id": "-Zblbo5WdoNK"
      },
      "id": "-Zblbo5WdoNK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CODEBERT_tokenizer_tag_v2(row):\n",
        "    mode = row.cwetype\n",
        "    content = row.short_text\n",
        "\n",
        "    new_info = []\n",
        "    if 'cdef ' in content[:50]:\n",
        "        return []\n",
        "    if len(content) > 100000:\n",
        "        return []\n",
        "\n",
        "    token_dict = tokenizer(content.strip(), truncation=False)\n",
        "    input_ids = token_dict['input_ids']\n",
        "    attention_mask = token_dict['attention_mask']\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_dict.input_ids)\n",
        "    token_list = [tok for tok in tokens if tok != '<s>' and tok != '<pad>' and tok != '</s>']\n",
        "    tok_results = []\n",
        "\n",
        "    if row.type == 'bp':\n",
        "        bp = row.parts\n",
        "\n",
        "        bp_token_dict = tokenizer(bp.strip(), truncation = False)\n",
        "        bp_tokens = tokenizer.convert_ids_to_tokens(bp_token_dict.input_ids)\n",
        "        bp_token_list = [bp_tok for bp_tok in bp_tokens if bp_tok != '<s>' and bp_tok != '<pad>' and bp_tok != '</s>']\n",
        "\n",
        "        if bp_token_list == [] or len(set(bp_token_list)) == 1:\n",
        "            return tok_results\n",
        "        else:\n",
        "            if bp_token_list != [] and len(bp_token_list) > 1:\n",
        "                while bp_token_list[-1] == '':\n",
        "                    bp_token_list.pop()\n",
        "\n",
        "        tester = []\n",
        "        for i in range(len(token_list)-len(bp_token_list)):\n",
        "            # Not looking at the first and last because that was causing mis-match issues\n",
        "            if token_list[i:i+len(bp_token_list)] == bp_token_list:\n",
        "                tester.append(i)\n",
        "                tester.append(i+len(bp_token_list))\n",
        "\n",
        "\n",
        "        if tester == []:\n",
        "\n",
        "            bp_token_list[0] = 'Ä '+bp_token_list[0]\n",
        "            tester2 = []\n",
        "            for i in range(len(token_list)-len(bp_token_list)):\n",
        "                if token_list[i:i+len(bp_token_list)] == bp_token_list:\n",
        "                    tester2.append(i)\n",
        "                    tester2.append(i+len(bp_token_list))\n",
        "            if tester2 == []:\n",
        "                tester3 = []\n",
        "                for i in range(len(token_list)-len(bp_token_list)):\n",
        "                    if token_list[i+1:i+len(bp_token_list)] == bp_token_list[1:]:\n",
        "                        tester3.append(i)\n",
        "                        tester3.append(i+len(bp_token_list))\n",
        "                if tester3 == []:\n",
        "                    return tok_results\n",
        "                else:\n",
        "                    ner_tags = tag_vulparts_v2(token_list, mode, tester3, token_dict)\n",
        "            else:\n",
        "              ner_tags = tag_vulparts_v2(token_list, mode, tester2, token_dict)\n",
        "        else:\n",
        "            ner_tags = tag_vulparts_v2(token_list, mode, tester, token_dict)\n",
        "\n",
        "        token_list.append('</s>')\n",
        "        token_list.insert(0, '<s>')\n",
        "        ner_tags.append(-100)\n",
        "        ner_tags.insert(0, -100)\n",
        "        tok_results.append({\"ner_tags\": ner_tags, 'tokens': token_list,\n",
        "                            'input_ids':input_ids, 'attention_mask':attention_mask})\n",
        "\n",
        "    else:\n",
        "        token_list.append('</s>')\n",
        "        token_list.insert(0, '<s>')\n",
        "        ner_tags = [0]*len(token_list)\n",
        "        tok_results.append({\"ner_tags\": ner_tags, 'tokens': token_list,\n",
        "                            'input_ids':input_ids, 'attention_mask':attention_mask})\n",
        "\n",
        "    return tok_results"
      ],
      "metadata": {
        "id": "rjpRsEq3d20w"
      },
      "id": "rjpRsEq3d20w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shorten_data_with_windows_after_tokenization(infodict):\n",
        "    '''\n",
        "    Shortening after tokenization\n",
        "    '''\n",
        "    new_info = []\n",
        "    new_size = 510\n",
        "\n",
        "    tags = infodict['ner_tags']\n",
        "    tokens = infodict['tokens']\n",
        "    attn = infodict['attention_mask']\n",
        "    input_ids = infodict['input_ids']\n",
        "    tag_indeces = [i for i, x in enumerate(tags) if x != 0 and x != -100]\n",
        "\n",
        "    # If there are more than 512 tokens\n",
        "    if len(tokens) >= new_size:\n",
        "        # If the indeces start and stop before 512, cut the list to 512\n",
        "        if tag_indeces[0] < new_size and tag_indeces[-1] <= new_size:\n",
        "            breaks = [i for i, x in enumerate(tokens) if '\\n' in x or ')' in x or ']' in x or '}' in x or '\\t' in x]\n",
        "            if [m for m in breaks if m <= new_size] == []:\n",
        "                new_info.append({'new_tokens': [], 'new_tags': [],\n",
        "                            'new_attn': [], 'new_input_ids': []})\n",
        "                return new_info\n",
        "            else:\n",
        "                break_id_to_use = [m for m in breaks if m <= new_size][-1]\n",
        "                new_tokens = tokens[:break_id_to_use]\n",
        "                new_tags = tags[:break_id_to_use]\n",
        "                new_attn = attn[:break_id_to_use]\n",
        "                new_input_ids = input_ids[:break_id_to_use]\n",
        "                new_info.append({'new_tokens': new_tokens, 'new_tags': new_tags,\n",
        "                                'new_attn': new_attn, 'new_input_ids': new_input_ids})\n",
        "\n",
        "        elif len(tag_indeces) <= new_size:\n",
        "            breaks = [i for i, x in enumerate(tokens) if '\\n' in x or ')' in x or ']' in x or '}' in x]\n",
        "            if [m for m in breaks if m <= tag_indeces[0]] == []:\n",
        "                new_info.append({'new_tokens': [], 'new_tags': [],\n",
        "                            'new_attn': [], 'new_input_ids': []})\n",
        "                return new_info\n",
        "            else:\n",
        "                break_id_to_use1 = [m for m in breaks if m <= tag_indeces[0]][-1]\n",
        "                break_id_to_use2 = [m for m in breaks if m >= break_id_to_use1 and m <= break_id_to_use1+new_size][-1]\n",
        "                new_tokens = tokens[break_id_to_use1:break_id_to_use2]\n",
        "                new_tags = tags[break_id_to_use1:break_id_to_use2]\n",
        "                new_attn = attn[break_id_to_use1:break_id_to_use2]\n",
        "                new_input_ids = input_ids[break_id_to_use1:break_id_to_use2]\n",
        "                new_info.append({'new_tokens': new_tokens, 'new_tags':new_tags,\n",
        "                                'new_attn': new_attn, 'new_input_ids': new_input_ids})\n",
        "        else:\n",
        "            breaks = [i for i, x in enumerate(tokens) if '\\n' in x or ')' in x or ']' in x or '}' in x]\n",
        "            break_ids_to_use = []\n",
        "            for i in range(math.ceil(len(tokens)/new_size)):\n",
        "                if i == 0:\n",
        "                    if [m for m in breaks if m <=new_size] == []:\n",
        "                        break_id = breaks[0]\n",
        "                    else:\n",
        "                        break_id = [m for m in breaks if m <=new_size][-1]\n",
        "                    chunk_start = 0\n",
        "                else:\n",
        "                    break_id = [m for m in breaks if m >= break_ids_to_use[i-1] and m <= break_ids_to_use[i-1]+new_size][-1]\n",
        "                    chunk_start = break_ids_to_use[i-1]\n",
        "                break_ids_to_use.append(break_id)\n",
        "                chunk = [chunk_start, break_id]\n",
        "                new_tokens = tokens[chunk[0]:chunk[1]]\n",
        "                new_tags = tags[chunk[0]:chunk[1]]\n",
        "                new_attn = attn[chunk[0]:chunk[1]]\n",
        "                new_input_ids = input_ids[chunk[0]:chunk[1]]\n",
        "                new_info.append({'new_tokens': new_tokens, 'new_tags':new_tags,\n",
        "                                'new_attn': new_attn, 'new_input_ids': new_input_ids})\n",
        "\n",
        "    else:\n",
        "        ## If there are not more than 512 tokens\n",
        "        new_tokens = tokens\n",
        "        new_tags = tags\n",
        "        new_attn = attn\n",
        "        new_input_ids = input_ids\n",
        "        new_info.append({'new_tokens': new_tokens, 'new_tags':new_tags,\n",
        "                        'new_attn': new_attn, 'new_input_ids': new_input_ids})\n",
        "\n",
        "    return new_info"
      ],
      "metadata": {
        "id": "sP6963fnd2sI"
      },
      "id": "sP6963fnd2sI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labels(row):\n",
        "    tags = row['ner_tags']\n",
        "    labels = []\n",
        "    to_change = tags\n",
        "    for x in to_change:\n",
        "        x[0] = -100\n",
        "        x[-1] = -100\n",
        "        pad = 512 - len(x)\n",
        "        x += [0]*pad\n",
        "        labels.append(x)\n",
        "    return {'labels':labels}\n",
        "\n",
        "def change_attention(row):\n",
        "    attention = row['attention_mask']\n",
        "    atts = []\n",
        "    to_change = attention\n",
        "    for x in to_change:\n",
        "        if 0 in x:\n",
        "            indx_to_change = x.index(0)\n",
        "            x[indx_to_change-1] = 0\n",
        "        else:\n",
        "            x[-1] = 0\n",
        "        x[0] = 0\n",
        "        atts.append(x)\n",
        "    return {'attention_mask':atts}"
      ],
      "metadata": {
        "id": "BR9w9DSld-S6"
      },
      "id": "BR9w9DSld-S6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DistilBERT_tokenizer_tag(row):\n",
        "    mode = row.cwetype\n",
        "    content = row.short_text\n",
        "\n",
        "    tok_results = []\n",
        "\n",
        "    token_dict = tokenizer(content.strip(), truncation=True, max_length = 512,  padding='max_length')\n",
        "    input_ids = token_dict['input_ids']\n",
        "    attention_mask = token_dict['attention_mask']\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_dict.input_ids)\n",
        "\n",
        "    token_list = [tok for tok in tokens if tok != '[SEP]' and tok != '[PAD]' and tok != '[CLS]']\n",
        "\n",
        "    if row.type == 'bp':\n",
        "        bp = row.parts\n",
        "\n",
        "        bp_token_dict = tokenizer(bp.strip(), max_length = 512, truncation = True)\n",
        "        bp_tokens = tokenizer.convert_ids_to_tokens(bp_token_dict.input_ids)\n",
        "        bp_token_list = [bp_tok for bp_tok in bp_tokens if bp_tok != '[SEP]' and bp_tok != '[PAD]' and bp_tok != '[CLS]']\n",
        "\n",
        "        if bp_token_list == [] or len(set(bp_token_list)) == 1:\n",
        "            return tok_results\n",
        "        else:\n",
        "            if bp_token_list != [] and len(bp_token_list) > 1:\n",
        "                while bp_token_list[-1] == '':\n",
        "                    bp_token_list.pop()\n",
        "\n",
        "        tester = []\n",
        "        for i in range(len(token_list)-len(bp_token_list)):\n",
        "            # Not looking at the first and last because that was causing mis-match issues\n",
        "            if token_list[i:i+len(bp_token_list)] == bp_token_list:\n",
        "                tester.append(i)\n",
        "                tester.append(i+len(bp_token_list))\n",
        "\n",
        "        if tester == []:\n",
        "\n",
        "            tester3 = []\n",
        "            for i in range(len(token_list)-len(bp_token_list)):\n",
        "                if token_list[i+1:i+len(bp_token_list)+1] == bp_token_list:\n",
        "                    tester3.append(i)\n",
        "                    tester3.append(i+len(bp_token_list))\n",
        "            if tester3 == []:\n",
        "                return tok_results\n",
        "            else:\n",
        "                ner_tags = tag_vulparts(token_list, mode, tester3)\n",
        "        else:\n",
        "            ner_tags = tag_vulparts(token_list, mode, tester)\n",
        "\n",
        "        tok_results.append({\"ner_tags\": ner_tags, 'tokens': token_list,\n",
        "                            'input_ids':input_ids, 'attention_mask':attention_mask})\n",
        "\n",
        "    else:\n",
        "        ner_tags = [0]*len(token_list)\n",
        "        tok_results.append({\"ner_tags\": ner_tags, 'tokens': token_list,\n",
        "                            'input_ids':input_ids, 'attention_mask':attention_mask})\n",
        "\n",
        "    return tok_results"
      ],
      "metadata": {
        "id": "LO7LUmeielb3"
      },
      "id": "LO7LUmeielb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d470972e-4d33-495b-852a-c87ffa2b842b",
      "metadata": {
        "tags": [],
        "id": "d470972e-4d33-495b-852a-c87ffa2b842b"
      },
      "source": [
        "## Import Collected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d7cefe-66ff-4964-9872-a5b7aa40e9d1",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "c3d7cefe-66ff-4964-9872-a5b7aa40e9d1"
      },
      "outputs": [],
      "source": [
        "file_list = list(os.listdir('Data\\\\'))\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for file in file_list:\n",
        "    with open('Data\\\\'+file, 'r') as infile:\n",
        "        data = json.load(infile)\n",
        "    if len(data) != 0:\n",
        "        print(file, len(data))\n",
        "        for info in data:\n",
        "            all_data.append(info)\n",
        "print(len(all_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95afbef2-40ac-4264-adab-11957e39fdf7",
      "metadata": {
        "id": "95afbef2-40ac-4264-adab-11957e39fdf7"
      },
      "outputs": [],
      "source": [
        "labeled_dict_list = []\n",
        "step1_dict_list = []\n",
        "toolongdict = dict()\n",
        "\n",
        "slightly_cleaned_all_data = [x for x in all_data if '<html' not in x['orig_txt']]\n",
        "slightly_cleaned_all_data = [x for x in slightly_cleaned_all_data if 'Search.setIndex' not in x['orig_txt']]\n",
        "slightly_cleaned_all_data = [x for x in slightly_cleaned_all_data if 'JavaScript Library' not in x['orig_txt']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning"
      ],
      "metadata": {
        "id": "DUudZyykugL7"
      },
      "id": "DUudZyykugL7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96cfacc1-66f5-4a8c-a609-34d99578493c",
      "metadata": {
        "id": "96cfacc1-66f5-4a8c-a609-34d99578493c"
      },
      "outputs": [],
      "source": [
        "df_all = pd.DataFrame(slightly_cleaned_all_data)\n",
        "df_new = df_all.copy()\n",
        "df_new = df_new[['cwetype', 'commit', 'neutralparts', 'vulparts','orig_diff','orig_txt']]\n",
        "df_new.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0980e3-aca6-4362-891f-fb6717b74ae8",
      "metadata": {
        "id": "9c0980e3-aca6-4362-891f-fb6717b74ae8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "neutraltext = [getneutralText(x) for x in df_new.orig_diff]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa0751e-7858-4203-9843-a4605d139740",
      "metadata": {
        "id": "aaa0751e-7858-4203-9843-a4605d139740"
      },
      "outputs": [],
      "source": [
        "## VUL PARTS ##\n",
        "df_bp = df_new.copy()\n",
        "df_bp = df_bp[['cwetype', 'vulparts', 'orig_txt']]\n",
        "\n",
        "# find all single comments and replace with nothing\n",
        "bp_text_without_single_comments = [re.sub('(#.*)', '', str(x)) for x in df_bp.orig_txt.tolist()]\n",
        "# find all comment blocks and replace with nothing - double quotes\n",
        "bp_text_without_comment_blocks1 = [re.sub(r'([\"])\\1\\1(.*?)\\1{3}', '', str(x), flags = re.DOTALL) for x in bp_text_without_single_comments]\n",
        "# find all comment blocks and replace with nothing - single quotes\n",
        "bp_text_without_comment_blocks2 = [re.sub(r\"(['])\\1\\1(.*?)\\1{3}\", ' ', str(x), flags = re.DOTALL) for x in bp_text_without_comment_blocks1]\n",
        "\n",
        "df_bp['text'] = bp_text_without_comment_blocks2\n",
        "df_bp = df_bp.drop(columns =['orig_txt'])\n",
        "df_bp['type'] = ['bp']*len(df_bp)\n",
        "df_bp.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e51a74f-972c-4135-a07e-71456e25fe8d",
      "metadata": {
        "id": "5e51a74f-972c-4135-a07e-71456e25fe8d"
      },
      "outputs": [],
      "source": [
        "df_bp = df_bp.rename(columns = {'vulparts':'parts'})\n",
        "df = df_bp.copy()\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fcfe57-f569-4e1b-80e8-6862de155453",
      "metadata": {
        "id": "f2fcfe57-f569-4e1b-80e8-6862de155453"
      },
      "outputs": [],
      "source": [
        "#saving just in case\n",
        "import pickle\n",
        "with open('clean_dataset.pickle', 'wb') as output:\n",
        "    pickle.dump(df, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e9c89e-1a11-4d73-8f02-ce1d17965df9",
      "metadata": {
        "id": "67e9c89e-1a11-4d73-8f02-ce1d17965df9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('clean_dataset.pickle', 'rb') as input:\n",
        "    df = pickle.load(input)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6adb66fb-4eb0-46f2-8134-f1d3ecbd528a",
      "metadata": {
        "id": "6adb66fb-4eb0-46f2-8134-f1d3ecbd528a"
      },
      "outputs": [],
      "source": [
        "df = df.explode('parts')\n",
        "df = df.fillna('')\n",
        "df = df[df.text != '']\n",
        "df = df[df.parts != '']\n",
        "df = df[df.parts.str.len() > 1]\n",
        "df['len_set_parts'] = df['parts'].apply(lambda x: len(set(x.strip())))\n",
        "df = df[df.len_set_parts > 3 ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cwe = []\n",
        "for x in zip(df.type.tolist(), df.cwetype.tolist()):\n",
        "  if x[0] == 'gp':\n",
        "    cwe.append('neutral')\n",
        "  else:\n",
        "    cwe.append(x[1])\n",
        "df['cwetype'] = cwe\n",
        "df.cwetype.value_counts()"
      ],
      "metadata": {
        "id": "xOEY5PIPluFs"
      },
      "id": "xOEY5PIPluFs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d9938d-d08a-4bfe-b4e3-1e531de91450",
      "metadata": {
        "id": "73d9938d-d08a-4bfe-b4e3-1e531de91450"
      },
      "outputs": [],
      "source": [
        "df['bp_len'] = [len(x) for x in df.parts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0c46eb-fee5-47fb-bef4-0df524eecd57",
      "metadata": {
        "id": "ec0c46eb-fee5-47fb-bef4-0df524eecd57"
      },
      "outputs": [],
      "source": [
        "def find_partstart(row):\n",
        "    if row.type == 'bp':\n",
        "        bp_index = row['text'].find(row['parts'])\n",
        "    else:\n",
        "        bp_index = ''\n",
        "    return bp_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848349db-1d43-4996-87e2-5b08c9746420",
      "metadata": {
        "id": "848349db-1d43-4996-87e2-5b08c9746420"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df['bp_index'] = df.apply(find_partstart, axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "results = []\n",
        "for x in range(0, len(df), 50000):\n",
        "  print(len(df)-x)\n",
        "  df_test = df.copy()\n",
        "  df_test = df_test.iloc[x: x+50000]\n",
        "  results.append(df_test.apply(shorten_data_with_windows_v2, axis =1))"
      ],
      "metadata": {
        "id": "vL7U23OAjt4g"
      },
      "id": "vL7U23OAjt4g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_results = [i for info in results for i in info]\n",
        "len(short_results)"
      ],
      "metadata": {
        "id": "cx9sIc6UpI2A"
      },
      "id": "cx9sIc6UpI2A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['short_text'] = short_results\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "o_C68igKKvC1"
      },
      "id": "o_C68igKKvC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.explode('short_text')\n",
        "print(len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fAXWtzwFplYL"
      },
      "id": "fAXWtzwFplYL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ykVV7MzAs-45"
      },
      "id": "ykVV7MzAs-45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.short_text !='']\n",
        "df = df.dropna(subset=['short_text'])\n",
        "df = df[df.parts != '']\n",
        "df = df[df.parts.str.len() >= 3]\n",
        "df = df.reset_index(drop=True)\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "b2kq3FZctr4d"
      },
      "id": "b2kq3FZctr4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['short_text2'] = [x.strip() for x in df.short_text.to_list()]"
      ],
      "metadata": {
        "id": "oUDonT5xLCJq"
      },
      "id": "oUDonT5xLCJq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = ['short_text'])\n",
        "df = df.rename(columns = {'short_text2':'short_text'})"
      ],
      "metadata": {
        "id": "CjjciD83LmDU"
      },
      "id": "CjjciD83LmDU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "COxBkcJsu8e3"
      },
      "id": "COxBkcJsu8e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving just in case\n",
        "import pickle\n",
        "with open('clean_dataset_short.pickle', 'wb') as output:\n",
        "    pickle.dump(df, output)"
      ],
      "metadata": {
        "id": "O1SH56BpMFxg"
      },
      "id": "O1SH56BpMFxg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#opening\n",
        "import pickle\n",
        "with open('clean_dataset_short.pickle', 'rb') as input:\n",
        "    df = pickle.load(input)"
      ],
      "metadata": {
        "id": "R046Bsj5MEVE"
      },
      "id": "R046Bsj5MEVE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CodeBERT OR RoBERTa Tokenizer"
      ],
      "metadata": {
        "id": "qXBFj_jyjBzy"
      },
      "id": "qXBFj_jyjBzy"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Change here for specific tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "GX71ae3NuZnj"
      },
      "id": "GX71ae3NuZnj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df = df[~df.short_text.str.startswith('cdef')]\n",
        "df = df[~df.parts.str.strip().str.startswith('#')]\n",
        "df = df.drop_duplicates()\n",
        "len(df)"
      ],
      "metadata": {
        "id": "gSwhKL-iP_qv"
      },
      "id": "gSwhKL-iP_qv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp = df.copy()\n",
        "df_bp = df_bp[df_bp.type == 'bp']\n",
        "df_bp.reset_index(inplace=True, drop=True)\n",
        "len(df_bp)"
      ],
      "metadata": {
        "id": "RDhHYtuFe-i6"
      },
      "id": "RDhHYtuFe-i6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tag_tok_results_bp = []\n",
        "for x in range(0, len(df_bp), 50000):\n",
        "  df_test = df_bp.copy()\n",
        "  df_test = df_test.iloc[x: x+50000]\n",
        "  tag_tok_results_bp.append(df_test.apply(CODEBERT_tokenizer_tag_v2, axis =1))\n",
        "  print(len(df_bp)-x)"
      ],
      "metadata": {
        "id": "8Q8YWJBNflDf"
      },
      "id": "8Q8YWJBNflDf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tok_results_bp_all = [y for x in tag_tok_results_bp for y in x]\n",
        "len(tag_tok_results_bp_all)"
      ],
      "metadata": {
        "id": "tgDgsBElg6Fj"
      },
      "id": "tgDgsBElg6Fj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bp_ner_tags = []\n",
        "bp_tokens = []\n",
        "bp_attention = []\n",
        "bp_input_ids = []\n",
        "for x in tag_tok_results_bp_all:\n",
        "  if len(x) == 0:\n",
        "    bp_ner_tags.append('None')\n",
        "    bp_tokens.append('None')\n",
        "    bp_attention.append('None')\n",
        "    bp_input_ids.append('None')\n",
        "  else:\n",
        "    for y in x:\n",
        "      bp_ner_tags.append(y['ner_tags'])\n",
        "      bp_tokens.append(y['tokens'])\n",
        "      bp_attention.append(y['attention_mask'])\n",
        "      bp_input_ids.append(y['input_ids'])"
      ],
      "metadata": {
        "id": "sse4vEr04doK"
      },
      "id": "sse4vEr04doK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bp_ner_tags), len(bp_tokens), len(bp_attention), len(bp_input_ids)"
      ],
      "metadata": {
        "id": "DL1FuaNVGaUR"
      },
      "id": "DL1FuaNVGaUR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp['ner_tags'] = bp_ner_tags\n",
        "df_bp['tokens'] = bp_tokens\n",
        "df_bp['attention_mask'] = bp_attention\n",
        "df_bp['input_ids'] = bp_input_ids\n",
        "\n",
        "df_bp = df_bp.drop(columns = ['bp_len'])"
      ],
      "metadata": {
        "id": "FGzbVatM2PYF"
      },
      "id": "FGzbVatM2PYF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp = df_bp[df_bp.ner_tags != 'None']\n",
        "df_bp.reset_index(drop=True, inplace=True)\n",
        "df_bp.head(2)"
      ],
      "metadata": {
        "id": "JVSUGt--3seV"
      },
      "id": "JVSUGt--3seV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving just in case\n",
        "import pickle\n",
        "with open('dataset_short_tag_tok_bp.pickle', 'wb') as output:\n",
        "    pickle.dump(df_bp, output)"
      ],
      "metadata": {
        "id": "lE9rXWjygoiL"
      },
      "id": "lE9rXWjygoiL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3915359a-9e86-446b-9900-fbf6bb87924e",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "3915359a-9e86-446b-9900-fbf6bb87924e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('dataset_short_tag_tok_bp.pickle', 'rb') as input:\n",
        "    df_bp = pickle.load(input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp = df_bp.drop(columns = ['text'])\n",
        "df_bp = df_bp[['cwetype', 'type', 'short_text', 'parts', 'ner_tags', 'tokens', 'attention_mask','input_ids']]"
      ],
      "metadata": {
        "id": "5NcFFRuIccbg"
      },
      "id": "5NcFFRuIccbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "results2 = []\n",
        "for x in range(0, len(df_bp), 100000):\n",
        "  print(len(df_bp)-x)\n",
        "  df_test = df_bp.copy()\n",
        "  df_test = df_test.iloc[x: x+100000]\n",
        "  results2.append(df_test.apply(shorten_data_with_windows_after_tokenization, axis =1))"
      ],
      "metadata": {
        "id": "ZLFih7ku4DKt"
      },
      "id": "ZLFih7ku4DKt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_results2 = [i for info in results2 for i in info]\n",
        "len(short_results2)"
      ],
      "metadata": {
        "id": "20rDQBn40iUf"
      },
      "id": "20rDQBn40iUf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df_bp.copy()\n",
        "df_new['short_results'] = short_results2\n",
        "\n",
        "df_new = df_new.explode('short_results')\n",
        "print(len(df_new))\n",
        "df_new.head()"
      ],
      "metadata": {
        "id": "MRR2DmSX3Iu6"
      },
      "id": "MRR2DmSX3Iu6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ner_tags = []\n",
        "new_tokens = []\n",
        "new_attention = []\n",
        "new_input_ids = []\n",
        "for x in df_new.short_results.to_list():\n",
        "    if x['new_tags'] == []:\n",
        "        new_ner_tags.append('None')\n",
        "        new_tokens.append('None')\n",
        "        new_attention.append('None')\n",
        "        new_input_ids.append('None')\n",
        "    else:\n",
        "        if len(x['new_tags']) == len(x['new_tokens']) == len(x['new_attn']) == len(x['new_input_ids']):\n",
        "            new_ner_tags.append(x['new_tags'])\n",
        "            new_tokens.append(x['new_tokens'])\n",
        "            new_attention.append(x['new_attn'])\n",
        "            new_input_ids.append(x['new_input_ids'])\n",
        "        else:\n",
        "            new_ner_tags.append('None')\n",
        "            new_tokens.append('None')\n",
        "            new_attention.append('None')\n",
        "            new_input_ids.append('None')"
      ],
      "metadata": {
        "id": "UoW58Iry2ysm"
      },
      "id": "UoW58Iry2ysm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['new_tags'] = new_ner_tags\n",
        "df_new['new_tokens'] = new_tokens\n",
        "df_new['new_attn'] = new_attention\n",
        "df_new['new_input_ids'] = new_input_ids"
      ],
      "metadata": {
        "id": "ECy_Bgpf8vMU"
      },
      "id": "ECy_Bgpf8vMU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head(1)"
      ],
      "metadata": {
        "id": "Nfkicwm79w2Y"
      },
      "id": "Nfkicwm79w2Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df_new[df_new.new_tokens != 'None']\n",
        "df_new = df_new.reset_index(drop=True)\n",
        "\n",
        "print(len(df_new))"
      ],
      "metadata": {
        "id": "HHWnsmxU_az4"
      },
      "id": "HHWnsmxU_az4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = ['short_text'])\n",
        "df = df.rename(columns = {'short_text2':'short_text'})"
      ],
      "metadata": {
        "id": "jNZycM3n4DHH"
      },
      "id": "jNZycM3n4DHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.cwetype.value_counts()"
      ],
      "metadata": {
        "id": "f3q53Ttod-SN"
      },
      "id": "f3q53Ttod-SN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['len_set_parts'] = df_new['parts'].apply(lambda x: len(set(x)))"
      ],
      "metadata": {
        "id": "rU9ojW2jokBd"
      },
      "id": "rU9ojW2jokBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp_50 = df_new.copy()\n",
        "df_bp_50 = df_bp_50[df_bp_50.parts.str.len() >= 50]"
      ],
      "metadata": {
        "id": "PjZMIanXmbw6"
      },
      "id": "PjZMIanXmbw6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp_50['parts_stripped'] = df_bp_50['parts'].apply(lambda x: x.strip())\n",
        "df_bp_50['len_set_parts'] = df_bp_50['parts_stripped'].apply(lambda x: len(set(x)))"
      ],
      "metadata": {
        "id": "BHwxJMjzqCnp"
      },
      "id": "BHwxJMjzqCnp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_bp_50[df_bp_50.len_set_parts > 5])"
      ],
      "metadata": {
        "id": "3pyj5eU4pMyQ"
      },
      "id": "3pyj5eU4pMyQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp_50 = df_bp_50[df_bp_50.len_set_parts >= 5]"
      ],
      "metadata": {
        "id": "tnfee4HkqU2J"
      },
      "id": "tnfee4HkqU2J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_bp_50.cwetype.value_counts())"
      ],
      "metadata": {
        "id": "ggRBEpbGBmDg"
      },
      "id": "ggRBEpbGBmDg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp_50['len_set_toks'] = df_bp_50['new_tokens'].apply(lambda x: len(x))\n",
        "len(df_bp_50[df_bp_50.len_set_toks > 5])"
      ],
      "metadata": {
        "id": "3lSzgFyTCgUZ"
      },
      "id": "3lSzgFyTCgUZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.random import sample_without_replacement\n",
        "neutrals_to_take = sample_without_replacement(len(df_gp), 16480, random_state = 2023)\n",
        "neutrals_to_take.sort()\n",
        "neutrals_to_take = neutrals_to_take.tolist()\n",
        "len(df_gp.iloc[neutrals_to_take])\n",
        "df_gp_16480 = df_gp.iloc[neutrals_to_take]"
      ],
      "metadata": {
        "id": "-IqQZRLM_Zca"
      },
      "id": "-IqQZRLM_Zca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_bp_50.copy()\n",
        "print(len(df_final))\n",
        "print(df_final.cwetype.value_counts())\n",
        "df_final.head(2)"
      ],
      "metadata": {
        "id": "D3_5MgFAccYJ"
      },
      "id": "D3_5MgFAccYJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving just in case\n",
        "import pickle\n",
        "with open('df_tagsandtokens.pickle', 'wb') as output:\n",
        "    pickle.dump(df_final, output)"
      ],
      "metadata": {
        "id": "sBGS7NQGgTnY"
      },
      "id": "sBGS7NQGgTnY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_final.drop(columns = ['ner_len','len_set_parts', 'parts_stripped'])"
      ],
      "metadata": {
        "id": "BUfKaI2mqy4V"
      },
      "id": "BUfKaI2mqy4V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_final.cwetype.value_counts())"
      ],
      "metadata": {
        "id": "yHUsIaFU0-_9"
      },
      "id": "yHUsIaFU0-_9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_final)"
      ],
      "metadata": {
        "id": "ORAie5xrNaXC"
      },
      "id": "ORAie5xrNaXC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "count = Counter()\n",
        "\n",
        "for row in df_final.new_tags.tolist():\n",
        "  for item in row:\n",
        "    count[item] += 1"
      ],
      "metadata": {
        "id": "OVOohJv7aO0Z"
      },
      "id": "OVOohJv7aO0Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count.most_common(13)"
      ],
      "metadata": {
        "id": "3lzgdC01aOMn"
      },
      "id": "3lzgdC01aOMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "lve40sY3fYW_"
      },
      "id": "lve40sY3fYW_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train, test = train_test_split(df_final, test_size=0.4, random_state=2023, shuffle = True, stratify = df_final.cwetype)"
      ],
      "metadata": {
        "id": "dRCTTrOufagh"
      },
      "id": "dRCTTrOufagh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print('train counts:\\n', train.cwetype.value_counts())\n",
        "train_data = datasets.Dataset.from_pandas(train)\n",
        "print('len of train data', len(train_data))\n",
        "print()\n",
        "\n",
        "valid, test2 = train_test_split(test, test_size=0.5, random_state=2023, shuffle = True, stratify = test.cwetype)\n",
        "print('test counts:\\n',test2.cwetype.value_counts())\n",
        "test_data = datasets.Dataset.from_pandas(test2)\n",
        "print('len of test data', len(test_data))\n",
        "print()\n",
        "\n",
        "print('valid counts:\\n', valid.cwetype.value_counts())\n",
        "valid_data = datasets.Dataset.from_pandas(valid)\n",
        "print('len of valid data', len(valid_data))\n",
        "print()\n",
        "\n",
        "dataset = datasets.DatasetDict({\"train\":train_data,'validation':valid_data, 'test':test_data})\n",
        "dataset"
      ],
      "metadata": {
        "id": "7XvmDPv8fNQD"
      },
      "id": "7XvmDPv8fNQD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_labels = dataset.map(create_labels, batched=True)"
      ],
      "metadata": {
        "id": "wonMnAaBleLO"
      },
      "id": "wonMnAaBleLO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_v2 = dataset_labels.map(change_attention, batched=True)"
      ],
      "metadata": {
        "id": "YbkWoRregIc3"
      },
      "id": "YbkWoRregIc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_v2"
      ],
      "metadata": {
        "id": "F131MWqitvsZ"
      },
      "id": "F131MWqitvsZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc597130-ba93-4f89-a2b7-73d0c980350a",
      "metadata": {
        "id": "dc597130-ba93-4f89-a2b7-73d0c980350a"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('PADDED_Roberta.pickle', 'wb') as output:\n",
        "    pickle.dump(dataset_v2, output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dataset_tagsandtokens.pickle', 'rb') as input:\n",
        "    dataset = pickle.load(input)"
      ],
      "metadata": {
        "id": "_4bEv0GkleYy"
      },
      "id": "_4bEv0GkleYy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilBERT Tokenizer"
      ],
      "metadata": {
        "id": "NLxq3CM3MNRf"
      },
      "id": "NLxq3CM3MNRf"
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "rGQ0XTabMPk-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rGQ0XTabMPk-"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df = df[~df.short_text.str.startswith('cdef')]\n",
        "df = df[~df.parts.str.strip().str.startswith('#')]\n",
        "df = df.drop_duplicates()\n",
        "len(df)"
      ],
      "metadata": {
        "id": "JaTIaHPwMPk-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JaTIaHPwMPk-"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp = df.copy()\n",
        "df_bp = df_bp[df_bp.type == 'bp']\n",
        "df_bp.reset_index(inplace=True, drop=True)\n",
        "len(df_bp)"
      ],
      "metadata": {
        "id": "khZmVs6sMPk_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "khZmVs6sMPk_"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp.cwetype.value_counts()"
      ],
      "metadata": {
        "id": "2vVBUTB-MPk_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2vVBUTB-MPk_"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tag_tok_results_bp = []\n",
        "for x in range(0, len(df_bp), 25000):\n",
        "  df_test = df_bp.copy()\n",
        "  df_test = df_test.iloc[x: x+25000]\n",
        "  tag_tok_results_bp.append(df_test.apply(DistilBERT_tokenizer_tag, axis =1))\n",
        "  print(len(df_bp)-x)"
      ],
      "metadata": {
        "id": "8HwNn0WjMPlA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8HwNn0WjMPlA"
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tok_results_bp_all = [y for x in tag_tok_results_bp for y in x]\n",
        "len(tag_tok_results_bp_all)"
      ],
      "metadata": {
        "id": "cHTXBnI_MPlA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cHTXBnI_MPlA"
    },
    {
      "cell_type": "code",
      "source": [
        "bp_ner_tags = []\n",
        "bp_tokens = []\n",
        "bp_attention = []\n",
        "bp_input_ids = []\n",
        "for x in tag_tok_results_bp_all:\n",
        "  if len(x) == 0:\n",
        "    bp_ner_tags.append('None')\n",
        "    bp_tokens.append('None')\n",
        "    bp_attention.append('None')\n",
        "    bp_input_ids.append('None')\n",
        "  else:\n",
        "    for y in x:\n",
        "      bp_ner_tags.append(y['ner_tags'])\n",
        "      bp_tokens.append(y['tokens'])\n",
        "      bp_attention.append(y['attention_mask'])\n",
        "      bp_input_ids.append(y['input_ids'])"
      ],
      "metadata": {
        "id": "mTFj9_m2MPlB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mTFj9_m2MPlB"
    },
    {
      "cell_type": "code",
      "source": [
        "len(bp_ner_tags), len(bp_tokens), len(bp_attention), len(bp_input_ids)"
      ],
      "metadata": {
        "id": "hKuMK5YSMPlB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hKuMK5YSMPlB"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp['ner_tags'] = bp_ner_tags\n",
        "df_bp['tokens'] = bp_tokens\n",
        "df_bp['attention_mask'] = bp_attention\n",
        "df_bp['input_ids'] = bp_input_ids\n",
        "df_bp = df_bp.drop(columns = ['bp_len'])\n",
        "df_bp = df_bp[df_bp.ner_tags != 'None']\n",
        "df_bp.reset_index(drop=True, inplace=True)\n",
        "df_bp.head(3)"
      ],
      "metadata": {
        "id": "uUNesbTvMPlB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uUNesbTvMPlB"
    },
    {
      "cell_type": "code",
      "source": [
        "#saving just in case\n",
        "import pickle\n",
        "with open('dataset_short_tag_tok_bp.pickle', 'wb') as output:\n",
        "    pickle.dump(df_bp, output)"
      ],
      "metadata": {
        "id": "Q_DP_Vl3MPlC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Q_DP_Vl3MPlC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "AzvUiUXJMPlC"
      },
      "outputs": [],
      "source": [
        "with open('dataset_short_tag_tok_bp.pickle', 'rb') as input:\n",
        "    df_bp = pickle.load(input)"
      ],
      "id": "AzvUiUXJMPlC"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp = df_bp.drop(columns = ['text'])\n",
        "df_bp = df_bp[['cwetype', 'type', 'short_text', 'parts', 'ner_tags', 'tokens', 'attention_mask','input_ids']]\n",
        "df_bp.head()"
      ],
      "metadata": {
        "id": "yanshDISMPlC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yanshDISMPlC"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp.cwetype.value_counts()"
      ],
      "metadata": {
        "id": "6dOvLdUeMPlC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6dOvLdUeMPlC"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp['len_set_parts'] = df_bp['parts'].apply(lambda x: len(set(x)))"
      ],
      "metadata": {
        "id": "lz_kpHdOMPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lz_kpHdOMPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp_50 = df_bp.copy()\n",
        "df_bp_50 = df_bp_50[df_bp_50.parts.str.len() >= 50]"
      ],
      "metadata": {
        "id": "_Q_qGF0HMPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_Q_qGF0HMPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp_50['parts_stripped'] = df_bp_50['parts'].apply(lambda x: x.strip())\n",
        "df_bp_50['len_set_parts'] = df_bp_50['parts_stripped'].apply(lambda x: len(set(x)))"
      ],
      "metadata": {
        "id": "3ExABwkMMPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3ExABwkMMPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_bp_50[df_bp_50.len_set_parts > 5])"
      ],
      "metadata": {
        "id": "GiR8Sv-cMPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GiR8Sv-cMPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp_50 = df_bp_50[df_bp_50.len_set_parts >= 5]"
      ],
      "metadata": {
        "id": "-MjSwZBdMPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-MjSwZBdMPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_bp_50.cwetype.value_counts())"
      ],
      "metadata": {
        "id": "lxb2Pzw7MPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lxb2Pzw7MPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.random import sample_without_replacement\n",
        "neutrals_to_take = sample_without_replacement(len(df_gp), 16480, random_state = 2023)\n",
        "neutrals_to_take.sort()\n",
        "neutrals_to_take = neutrals_to_take.tolist()\n",
        "len(df_gp.iloc[neutrals_to_take])\n",
        "df_gp_16480 = df_gp.iloc[neutrals_to_take]"
      ],
      "metadata": {
        "id": "NloKHvYNMPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NloKHvYNMPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_bp_50.copy()\n",
        "print(len(df_final))\n",
        "print(df_final.cwetype.value_counts())\n",
        "df_final.head(2)"
      ],
      "metadata": {
        "id": "L9uMRhF1MPlD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "L9uMRhF1MPlD"
    },
    {
      "cell_type": "code",
      "source": [
        "#saving just in case\n",
        "import pickle\n",
        "with open('df_tagsandtokens.pickle', 'wb') as output:\n",
        "    pickle.dump(df_final, output)"
      ],
      "metadata": {
        "id": "YAHh5xdqMPlE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YAHh5xdqMPlE"
    },
    {
      "cell_type": "code",
      "source": [
        "df_final['ner_len'] = [len(x) for x in df_final.ner_tags]\n",
        "df_final = df_final[df_final.short_text != '']"
      ],
      "metadata": {
        "id": "dA6VTUjXMPlE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dA6VTUjXMPlE"
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_final.drop(columns = ['ner_len','len_set_parts', 'parts_stripped'])"
      ],
      "metadata": {
        "id": "Nza-Pe5_MPlE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Nza-Pe5_MPlE"
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head(1)"
      ],
      "metadata": {
        "id": "e0GX_GgEMPlE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "e0GX_GgEMPlE"
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_final.cwetype.value_counts())"
      ],
      "metadata": {
        "id": "3GCtxRiIMPlE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3GCtxRiIMPlE"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "count = Counter()\n",
        "\n",
        "for row in df_final.ner_tags.tolist():\n",
        "  for item in row:\n",
        "    count[item] += 1"
      ],
      "metadata": {
        "id": "C-idLsoNMPlF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "C-idLsoNMPlF"
    },
    {
      "cell_type": "code",
      "source": [
        "count.most_common(13)"
      ],
      "metadata": {
        "id": "vSy4S33PMPlF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vSy4S33PMPlF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "aF8Naq5vMPlF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aF8Naq5vMPlF"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train, test = train_test_split(df_final, test_size=0.4, random_state=2023, shuffle = True, stratify = df_final.cwetype)"
      ],
      "metadata": {
        "id": "FMu0rAXlMPlF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FMu0rAXlMPlF"
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print('train counts:\\n', train.cwetype.value_counts())\n",
        "train_data = datasets.Dataset.from_pandas(train)\n",
        "print('len of train data', len(train_data))\n",
        "print()\n",
        "\n",
        "valid, test2 = train_test_split(test, test_size=0.5, random_state=2023, shuffle = True, stratify = test.cwetype)\n",
        "print('test counts:\\n',test2.cwetype.value_counts())\n",
        "test_data = datasets.Dataset.from_pandas(test2)\n",
        "print('len of test data', len(test_data))\n",
        "print()\n",
        "\n",
        "print('valid counts:\\n', valid.cwetype.value_counts())\n",
        "valid_data = datasets.Dataset.from_pandas(valid)\n",
        "print('len of valid data', len(valid_data))\n",
        "print()\n",
        "\n",
        "dataset = datasets.DatasetDict({\"train\":train_data,'validation':valid_data, 'test':test_data})\n",
        "dataset"
      ],
      "metadata": {
        "id": "lCGXvC20MPlF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lCGXvC20MPlF"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_labels = dataset.map(create_labels, batched=True)"
      ],
      "metadata": {
        "id": "ZwQ4fKFAMPlF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZwQ4fKFAMPlF"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_v2 = dataset_labels.map(change_attention, batched=True)"
      ],
      "metadata": {
        "id": "Ibw_A49eMPlG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ibw_A49eMPlG"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_v2"
      ],
      "metadata": {
        "id": "3LpkQIbfMPlG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3LpkQIbfMPlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCGxshvNMPlG"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('PADDED_distilbert.pickle', 'wb') as output:\n",
        "    pickle.dump(dataset_v2, output)"
      ],
      "id": "TCGxshvNMPlG"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dataset_tagsandtokens.pickle', 'rb') as input:\n",
        "    dataset = pickle.load(input)"
      ],
      "metadata": {
        "id": "mfInLN2xMPlG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mfInLN2xMPlG"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxZSk78Sufx3"
      },
      "id": "sxZSk78Sufx3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development"
      ],
      "metadata": {
        "id": "VRLJrgC0uhTY"
      },
      "id": "VRLJrgC0uhTY"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers evaluate seqeval pynvml pyyaml h5py huggingface_hub"
      ],
      "metadata": {
        "id": "hph57h1Rujm5"
      },
      "id": "hph57h1Rujm5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import datasets\n",
        "import pickle\n",
        "from datasets import load_metric\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from pynvml import *\n",
        "import tensorflow as tf\n",
        "\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "tqdmn = tqdm.notebook.tqdm\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TFBertModel\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "\n",
        "import sklearn\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import (\n",
        "   AutoConfig,\n",
        "   AutoTokenizer,\n",
        "   TFAutoModelForTokenClassification,\n",
        "   AdamW,\n",
        "   AdamWeightDecay)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "KpF4WR-mul8O"
      },
      "id": "KpF4WR-mul8O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file = 'roberta_tagsandtokens_PADDED.pickle'\n",
        "file = 'distilbert_tagsandtokens_PADDED.pickle'\n",
        "#file = 'codebert_tagsandtokens_PADDED.pickle'\n",
        "with open(file, 'rb') as data:\n",
        "    dataset = pickle.load(data)\n",
        "dataset"
      ],
      "metadata": {
        "id": "iZW6KCD0uuWr"
      },
      "id": "iZW6KCD0uuWr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_choice = \"roberta-base\"\n",
        "tok_choice = \"roberta-base\"\n",
        "#model_choice = 'microsoft/codebert-base'\n",
        "#tok_choice = 'microsoft/codebert-base'\n",
        "#model_choice = \"distilbert-base-uncased\"\n",
        "#tok_choice = \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "7O48h1TtuuUF"
      },
      "id": "7O48h1TtuuUF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = ['O',      #0\n",
        "              'B-rce',  #1\n",
        "              'I-rce',  #2\n",
        "              'B-oob',  #3\n",
        "              'I-oob',  #4\n",
        "              'B-xss',  #5\n",
        "              'I-xss',  #6\n",
        "              'B-sql',  #7\n",
        "              'I-sql',  #8\n",
        "              'B-iiv',  #9\n",
        "              'I-iiv',  #10\n",
        "              'B-pat',  #11\n",
        "              'I-pat',  #12\n",
        "             ]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}"
      ],
      "metadata": {
        "id": "x8WjSnoouuRZ"
      },
      "id": "x8WjSnoouuRZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "Rm6sW70iuuOs"
      },
      "id": "Rm6sW70iuuOs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}"
      ],
      "metadata": {
        "id": "lviM_FqPuuMG"
      },
      "id": "lviM_FqPuuMG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_choice, num_labels = len(label_list))\n",
        "\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id"
      ],
      "metadata": {
        "id": "jgRXoxCVuuJo"
      },
      "id": "jgRXoxCVuuJo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "rQWBbnCxwf0J"
      },
      "id": "rQWBbnCxwf0J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(params=model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "gMcGLFr3uuHN"
      },
      "id": "gMcGLFr3uuHN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "wG_hOpmRuuEX"
      },
      "id": "wG_hOpmRuuEX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "9jJBwEPauuAv"
      },
      "id": "9jJBwEPauuAv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = DataLoader(\n",
        "    dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_data = DataLoader(\n",
        "    dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "wm_YB_luut7f"
      },
      "id": "wm_YB_luut7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 10\n",
        "num_training_steps = num_epochs * len(train_data)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ],
      "metadata": {
        "id": "JP8LzEl8ut5D"
      },
      "id": "JP8LzEl8ut5D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "model.train()\n",
        "for epoch in tqdmn(range(num_epochs)):\n",
        "    current_loss = 0\n",
        "    for i, batch in enumerate(tqdmn(train_data)):\n",
        "        # move the batch tensors to the same device as the\n",
        "        batch = { k: v.to(device) for k, v in batch.items() }\n",
        "        # send 'input_ids', 'attention_mask' and 'labels' to the model\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss #outputs[0]\n",
        "        loss.backward()\n",
        "        current_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.append(current_loss / 10)\n",
        "        current_loss = 0\n",
        "\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "Il0MOD55wxTt"
      },
      "id": "Il0MOD55wxTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "veR1hbxQwxRI"
      },
      "id": "veR1hbxQwxRI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('PyNERModel_dev')"
      ],
      "metadata": {
        "id": "SIJgsy9dwxOG"
      },
      "id": "SIJgsy9dwxOG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "whoami()"
      ],
      "metadata": {
        "id": "bF_HggoSxCxc"
      },
      "id": "bF_HggoSxCxc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(repo_id = '') #need to input user id"
      ],
      "metadata": {
        "id": "dZ1WCoIAwxLJ"
      },
      "id": "dZ1WCoIAwxLJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mw9sdLR5xb1l"
      },
      "id": "mw9sdLR5xb1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Test"
      ],
      "metadata": {
        "id": "tWTgtIL5yXPq"
      },
      "id": "tWTgtIL5yXPq"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub datasets transformers"
      ],
      "metadata": {
        "id": "u5iJZsQeyYN1"
      },
      "id": "u5iJZsQeyYN1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "0Ldcc70UybuX"
      },
      "id": "0Ldcc70UybuX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "#tok_choice = 'roberta-base'\n",
        "tok_choice = 'microsoft/codebert-base'\n",
        "#tok_choice = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tok_choice)"
      ],
      "metadata": {
        "id": "XlyFKFssybr_"
      },
      "id": "XlyFKFssybr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "ARW4sWh4ybpq"
      },
      "id": "ARW4sWh4ybpq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "model = AutoModelForTokenClassification.from_pretrained('mmeberg/CoCo_PyVulDet_NER')\n",
        "#'mmeberg/CoCo_PyVulDet_NER'\n",
        "#'mmeberg/RoCo_PyVulDet_NER'\n",
        "#'mmeberg/RoRo_PyVulDet_NER'\n",
        "#'mmeberg/CoRo_PyVulDet_NER'\n",
        "#'mmeberg/DiDi_PyVulDet_NER'"
      ],
      "metadata": {
        "id": "CNOBRkr2ybmv"
      },
      "id": "CNOBRkr2ybmv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "tqdmn = tqdm.notebook.tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "x_w87Dcuybhq"
      },
      "id": "x_w87Dcuybhq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "qdhVxBrwybe7"
      },
      "id": "qdhVxBrwybe7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "BEp1eb6YybcG"
      },
      "id": "BEp1eb6YybcG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "hmc-_B_XybU9"
      },
      "id": "hmc-_B_XybU9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GitHub Test Set"
      ],
      "metadata": {
        "id": "MvKAnoQByqmY"
      },
      "id": "MvKAnoQByqmY"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import datasets\n",
        "\n",
        "file = 'codebert_tagsandtokens_PADDED.pickle'\n",
        "with open(file, 'rb') as data:\n",
        "    dataset = pickle.load(data)\n",
        "dataset"
      ],
      "metadata": {
        "id": "KGOjmyKMyojz"
      },
      "id": "KGOjmyKMyojz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.DataFrame(dataset['test'])\n",
        "test_df = test_df[['input_ids', 'attention_mask', 'labels']]\n",
        "test_df.head(2)"
      ],
      "metadata": {
        "id": "POGiKsR4yogx"
      },
      "id": "POGiKsR4yogx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "xazVXtb9yod7"
      },
      "id": "xazVXtb9yod7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.set_format(type='torch')"
      ],
      "metadata": {
        "id": "kZ8XcCM9yob7"
      },
      "id": "kZ8XcCM9yob7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.format['type']"
      ],
      "metadata": {
        "id": "NERD1gbuyoZM"
      },
      "id": "NERD1gbuyoZM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "test_data = DataLoader(\n",
        "    test_data, shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "12LxEbGvyoRl"
      },
      "id": "12LxEbGvyoRl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = ['O',      #0\n",
        "              'B-rce',  #1\n",
        "              'I-rce',  #2\n",
        "              'B-oob',  #3\n",
        "              'I-oob',  #4\n",
        "              'B-xss',  #5\n",
        "              'I-xss',  #6\n",
        "              'B-sql',  #7\n",
        "              'I-sql',  #8\n",
        "              'B-iiv',  #9\n",
        "              'I-iiv',  #10\n",
        "              'B-pat',  #11\n",
        "              'I-pat',  #12\n",
        "             ]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}"
      ],
      "metadata": {
        "id": "ssH9eTJQyoOz"
      },
      "id": "ssH9eTJQyoOz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Expand results so no longer lists of lists\n",
        "for_cm_true = [y for x in all_true for y in x]\n",
        "print(len(for_cm_true))\n",
        "for_cm_pred = [y for x in all_pred for y in x]\n",
        "print(len(for_cm_pred))"
      ],
      "metadata": {
        "id": "FLhtl1tAy3nD"
      },
      "id": "FLhtl1tAy3nD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_name = list(label2id.keys())\n",
        "print(labels_name)\n",
        "labels_id = list(id2label.keys())\n",
        "print(labels_id)"
      ],
      "metadata": {
        "id": "7uTX_ILQy3ke"
      },
      "id": "7uTX_ILQy3ke",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn.metrics as skm\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "HZEKpaoTy3iI"
      },
      "id": "HZEKpaoTy3iI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_id_11 = list(id2label.keys())[1:]\n",
        "print(labels_id_11)\n",
        "labels_name_11 = list(label2id.keys())[1:]\n",
        "print(labels_name_11)"
      ],
      "metadata": {
        "id": "quKjireWy3fy"
      },
      "id": "quKjireWy3fy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(for_cm_true, for_cm_pred, labels =labels_id_11)"
      ],
      "metadata": {
        "id": "1qVh4_JSy3bO"
      },
      "id": "1qVh4_JSy3bO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize\n",
        "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=labels_name_11, yticklabels=labels_name_11)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "#plt.savefig('plot4.jpg', format='jpg', dpi=1200)\n",
        "plt.show(block=False)"
      ],
      "metadata": {
        "id": "udNqUVzBy3Yu"
      },
      "id": "udNqUVzBy3Yu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "confusion_np = cmn#.np()\n",
        "FP = confusion_np.sum(axis=0) - np.diag(confusion_np)\n",
        "FN = confusion_np.sum(axis=1) - np.diag(confusion_np)\n",
        "TP = np.diag(confusion_np)\n",
        "TN = confusion_np.sum() - (FP + FN + TP)\n",
        "\n",
        "# Sensitivity, hit rate, recall, or true positive rate\n",
        "TPR = TP/(TP+FN)\n",
        "# Specificity or true negative rate\n",
        "TNR = TN/(TN+FP)\n",
        "# Precision or positive predictive value\n",
        "PPV = TP/(TP+FP)\n",
        "# Negative predictive value\n",
        "NPV = TN/(TN+FN)\n",
        "# Fall out or false positive rate\n",
        "FPR = FP/(FP+TN)\n",
        "# False negative rate\n",
        "FNR = FN/(TP+FN)\n",
        "# False discovery rate\n",
        "FDR = FP/(TP+FP)\n",
        "#F1-Score\n",
        "F1 = TP/(TP+.5*(FP+FN))\n",
        "\n",
        "# Overall accuracy\n",
        "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "\n",
        "metrics = {'FP':FP, 'FN':FN, 'TP':TP, 'TN':TN, 'TPR':TPR, 'TNR':TNR, 'PPV':PPV, 'NPV':NPV, 'FPR':FPR, 'FNR':FNR, 'FDR':FDR, 'F1':F1, 'ACC':ACC}\n",
        "for k, v in metrics.items():\n",
        "    print(k)\n",
        "    for x in zip(labels_name_11, list(v)):\n",
        "        print(x[0], x[1])\n",
        "    print()"
      ],
      "metadata": {
        "id": "tWljByxUy3WE"
      },
      "id": "tWljByxUy3WE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tns = [\n",
        " 'rce', #1\n",
        " 'oob', #2\n",
        " 'xss', #3\n",
        " 'sql', #4\n",
        " 'iiv', #5\n",
        " 'pat'  #6\n",
        " ]\n",
        "\n",
        "test_tns_ids = [ 1, 2, 3, 4, 5, 6]\n",
        "\n",
        "cwes = [\n",
        " 'CWE-94', #1\n",
        " 'CWE-787', #2\n",
        " 'CWE-79', #3\n",
        " 'CWE-89', #4\n",
        " 'CWE-20', #5\n",
        " 'CWE-22'  #6\n",
        " ]\n",
        "\n",
        "\n",
        "id2label_test = {i: label for i, label in enumerate(test_tns,1)}\n",
        "label2id_test = {label: i for i, label in enumerate(test_tns,1)}\n",
        "\n",
        "all_true_test = [label2id_test[id2label[x].split('-')[1]] if x!=0 else 0 for x in for_cm_true]\n",
        "all_pred_test = [label2id_test[id2label[x].split('-')[1]] if x!=0 else 0 for x in for_cm_pred]\n",
        "\n",
        "#print(classification_report(all_true_test, all_pred_test, target_names=test_tns))"
      ],
      "metadata": {
        "id": "MuXMNjeezBRD"
      },
      "id": "MuXMNjeezBRD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm7 = confusion_matrix(all_true_test, all_pred_test, labels=test_tns_ids)\n",
        "cmn7 = cm7.astype('float') / cm7.sum(axis=1)[:, np.newaxis]"
      ],
      "metadata": {
        "id": "vpoIw05jzBO9"
      },
      "id": "vpoIw05jzBO9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion7_np = cmn7\n",
        "FP = confusion7_np.sum(axis=0) - np.diag(confusion7_np)\n",
        "FN = confusion7_np.sum(axis=1) - np.diag(confusion7_np)\n",
        "TP = np.diag(confusion7_np)\n",
        "TN = confusion7_np.sum() - (FP + FN + TP)\n",
        "\n",
        "\n",
        "# Sensitivity, hit rate, recall, or true positive rate\n",
        "TPR = TP/(TP+FN)\n",
        "# Specificity or true negative rate\n",
        "TNR = TN/(TN+FP)\n",
        "# Precision or positive predictive value\n",
        "PPV = TP/(TP+FP)\n",
        "# Negative predictive value\n",
        "NPV = TN/(TN+FN)\n",
        "# Fall out or false positive rate\n",
        "FPR = FP/(FP+TN)\n",
        "# False negative rate\n",
        "FNR = FN/(TP+FN)\n",
        "# False discovery rate\n",
        "FDR = FP/(TP+FP)\n",
        "#F1-Score\n",
        "F1 = TP/(TP+.5*(FP+FN))\n",
        "\n",
        "# Overall accuracy\n",
        "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "\n",
        "metrics = {'FP':FP, 'FN':FN, 'TP':TP, 'TN':TN, 'TPR':TPR, 'TNR':TNR, 'PPV':PPV, 'NPV':NPV, 'FPR':FPR, 'FNR':FNR, 'FDR':FDR, 'F1':F1, 'ACC':ACC}\n",
        "for k, v in metrics.items():\n",
        "    print(k)\n",
        "    for x in zip(test_tns, list(v)):\n",
        "        print(x[0], x[1])\n",
        "    print()"
      ],
      "metadata": {
        "id": "qoapjGRtzBMY"
      },
      "id": "qoapjGRtzBMY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.heatmap(cmn7, annot=True, fmt='.2f', xticklabels=cwes, yticklabels=cwes)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show(block=False)"
      ],
      "metadata": {
        "id": "0EB-tZgczBJu"
      },
      "id": "0EB-tZgczBJu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns1 = sns.heatmap(cmn7, annot=True, fmt='.2f', ax=ax1, xticklabels=cwes, yticklabels=cwes, annot_kws={\"size\": 15})\n",
        "plt.ylabel('Actual', fontsize=15)\n",
        "plt.xlabel('Predicted', fontsize=15)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "cbar1 = sns1.collections[0].colorbar\n",
        "cbar1.ax.tick_params(labelsize=14)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns2 = sns.heatmap(cmn, annot=True, fmt='.2f', ax=ax2, xticklabels=labels_name_11, yticklabels=labels_name_11, annot_kws={\"size\": 14})\n",
        "plt.ylabel('Actual', fontsize=15)\n",
        "plt.xlabel('Predicted', fontsize=15)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "cbar2 = sns2.collections[0].colorbar\n",
        "cbar2.ax.tick_params(labelsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('Figure_A1.jpg', format='jpg', dpi=1200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5xlNaPAjzBHJ"
      },
      "id": "5xlNaPAjzBHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoTME1-Xy3Tf"
      },
      "id": "BoTME1-Xy3Tf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displacy"
      ],
      "metadata": {
        "id": "azty93PxXvyk"
      },
      "id": "azty93PxXvyk"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import spacy\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "rTdhnVCJXw7n"
      },
      "id": "rTdhnVCJXw7n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_pipeline = AutoModelForTokenClassification.from_pretrained(\"mmeberg/CoCo_PyVulDet_NER\")\n",
        "token_classifier = pipeline(\"ner\", model = model_for_pipeline, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "vQnX9e-uX1Vl"
      },
      "id": "vQnX9e-uX1Vl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entities_html(text, ner_result, title=None):\n",
        "    \"\"\"Visualize NER with the help of SpaCy\"\"\"\n",
        "    ents = []\n",
        "    for ent in ner_result:\n",
        "        e = {}\n",
        "        e[\"start\"] = ent[\"start\"]\n",
        "        e[\"end\"] = ent[\"end\"]\n",
        "        # add the score if you want in the label\n",
        "        #e[\"label\"] = f\"{ent[\"entity\"]}-{ent['score']:.2f}\"\n",
        "        e[\"label\"] = ent[\"entity\"]\n",
        "        if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
        "            ents[-1][\"end\"] = e[\"end\"]\n",
        "            continue\n",
        "        ents.append(e)\n",
        "    render_data = [\n",
        "    {\n",
        "        \"text\": text,\n",
        "        \"ents\": ents,\n",
        "        \"title\": title,\n",
        "    }\n",
        "    ]\n",
        "    spacy.displacy.render(render_data, style=\"ent\", manual=True, jupyter=True)"
      ],
      "metadata": {
        "id": "H5DWGCU8X1Sq"
      },
      "id": "H5DWGCU8X1Sq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(id2label)\n",
        "print(label2id)"
      ],
      "metadata": {
        "id": "r9P90HGxX1P7"
      },
      "id": "r9P90HGxX1P7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "while t != 4:\n",
        "    i = random.randint(0, len(dataset['test']))\n",
        "    if int(dataset['test'][i]['labels'][1]) == 0:\n",
        "        t = 4\n",
        "        print(i)\n",
        "\n",
        "print(i)\n",
        "print(dataset['test'][i]['labels'][:200])\n",
        "print(dataset['test']['cwetype'][i])\n"
      ],
      "metadata": {
        "id": "phimWwqPX1NL"
      },
      "id": "phimWwqPX1NL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['test']['short_text'][i])"
      ],
      "metadata": {
        "id": "do5wFfn8X1KZ"
      },
      "id": "do5wFfn8X1KZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['test']['parts'][i])"
      ],
      "metadata": {
        "id": "aO7eMORqX1GN"
      },
      "id": "aO7eMORqX1GN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ner = token_classifier(text_to_test)\n",
        "get_entities_html(text_to_test, doc_ner)"
      ],
      "metadata": {
        "id": "SfKCEkkeYOHH"
      },
      "id": "SfKCEkkeYOHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lfouOFICYOEZ"
      },
      "id": "lfouOFICYOEZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}